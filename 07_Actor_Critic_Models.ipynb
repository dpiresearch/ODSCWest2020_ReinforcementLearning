{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07 Actor Critic Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpiresearch/ODSCWest2020_ReinforcementLearning/blob/main/07_Actor_Critic_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRSYKGg_N_PU"
      },
      "source": [
        "# Asynchronous Advantage Actor Critic\n",
        "\n",
        "Note, you must run it on jupyther notebook to visualize the results.\n",
        "\n",
        "## Overview\n",
        "In this tutorial we will see a few useful concepts:\n",
        "\n",
        "* A3C, one of the simplest and most effective Deep RL techniques, which basically made DQN obsolete.\n",
        "* Model subclassing: we will subclass tf.keras.Model and custumize our forward pass. Because of eager executing the forward pass can be written imperatively.\n",
        "\n",
        "We will need to:\n",
        "\n",
        "* Create a master agent supervisor\n",
        "* Create worker agents\n",
        "* Implement A3C\n",
        "* Train\n",
        "\n",
        "Paper: Asynchronous Methods for Deep Reinforcement Learning by Volodymyr Mnih\n",
        "\n",
        "## Task: CartPole\n",
        "Cartpole is a simple game in which the player needs to balance pole connected to a cart with un-actuated joint. The cart can move left or right. At the beginning cart position, velocity, pole angle, and velocity are randomly initialized between +/-0.05.\n",
        "\n",
        "The agent can apply a force of +1 or -1 to the cart which will move it left or right). The goals is to balance the pole acting on the cart.\n",
        "\n",
        "At every timestep in which the pole is upright the agent receive a positive reward of +1. If the pole is more than 15 degrees from vertical or the cart moves more than 2.4 units from the center the episode ends.\n",
        "\n",
        "## Actor critic model\n",
        "### Policy gradients\n",
        "The idea behind policy gradients is to parametrise the action probability distribution given some input state.in DL a network do the job: takes the state of the game (using raw pixels, RAM or others) output probability distributions. Then it will sample from the distributions and decides what we should do.\n",
        "\n",
        "More formally, policy gradients are a special case of the more general score function gradient estimator. The general case is expressed in the form of Expectation(x | ) [f(x)]:the expectation of the reward function f, under the policy p.Using the log derivative trick, we demonstrate that we can search for our network’s parameters to find the action samples that maximises rewards, just like for normal Deep Learning problems: ∇ Ex[f(x)] = Ex[f(x) ∇ log p(x)]. This last equation is saying that bu moving θ in the direction of the gradient we can maximize the value of the expected reward function.\n",
        "\n",
        "As we said the Value functions is telling us how good a certain state is, basically corresponds to the loss function of a supervised learning problem. The value function, the Critic part of the algorithm, is used by the agent uses This is where the “Critic” part of the name becomes relevant. The agent uses the value estimate (the critic) to update the policy (the actor).\n",
        "\n",
        "Implementation Let’s first define what kind of model we’ll be using. The master agent will have the global network and each local worker agent will have a copy of this network in their own process. We will instantiate the model using Kera's model class, which can be find in tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HudixQBFNgts"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "import threading\n",
        "import gym\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "from queue import Queue\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras import layers\n",
        "\n",
        "\n",
        "verbose = False\n",
        "\n",
        "class ActorCriticModel(keras.Model):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(ActorCriticModel, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.dense1 = layers.Dense(100, activation='relu') \n",
        "        self.policy_logits = layers.Dense(action_size)\n",
        "        self.dense2 = layers.Dense(100, activation='relu')\n",
        "        self.values = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Forward pass\n",
        "        x = 0 # Complete here\n",
        "        logits = 0 # Complete here\n",
        "        v1 = 0 # Complete here\n",
        "        values = 0 # Complete here\n",
        "        return logits, values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoyOx71ROGbv"
      },
      "source": [
        "### Random Agent \n",
        "To have a baseline we will create a random agent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqjzfwEOOEQH"
      },
      "source": [
        "class RandomAgent(object):\n",
        "    \"\"\"\n",
        "      env_name: OpenAi-s gym enviroment name.\n",
        "      max_eps: Maximum number of episodes the agent will run.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_name, max_eps):\n",
        "        self.env = gym.make(env_name)\n",
        "        self.max_episodes = max_eps\n",
        "        self.global_moving_average_reward = 0\n",
        "        self.res_queue = Queue()\n",
        "\n",
        "    def run(self):\n",
        "        reward_avg = 0\n",
        "        for episode in range(self.max_episodes):\n",
        "\n",
        "            done = False\n",
        "            self.env.reset()\n",
        "            reward_sum = 0.0\n",
        "            steps = 0\n",
        "            while not done:\n",
        "                # Sample randomly from the action space and step\n",
        "                _, reward, done, _ = self.env.step()  # Complete here\n",
        "                steps += 1\n",
        "                reward_sum += reward\n",
        "                # Record statistics\n",
        "                self.global_moving_average_reward = record(\n",
        "                    episode, reward_sum, 0, self.global_moving_average_reward,\n",
        "                    self.res_queue, 0, steps)\n",
        "\n",
        "                reward_avg += reward_sum\n",
        "\n",
        "        final_avg = reward_avg / float(self.max_episodes)\n",
        "        print(\"Average score across {} episodes: {}\".format(\n",
        "            self.max_episodes, final_avg))\n",
        "        return final_avg\n",
        "\n",
        "\n",
        "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue,\n",
        "           total_loss, num_steps):\n",
        "    \"\"\"Store score  statistics.\n",
        "    \"\"\"\n",
        "    if global_ep_reward == 0:\n",
        "        global_ep_reward = episode_reward\n",
        "    else:\n",
        "        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n",
        "        if verbose:\n",
        "            print(f\"Episode: {episode}:  \"\n",
        "                  f\"Episode Reward: {int(episode_reward)}:  \"\n",
        "                  f\"Steps: {num_steps}:  \"\n",
        "                  f\"Worker: {worker_idx}: \")\n",
        "    result_queue.put(global_ep_reward)\n",
        "    return global_ep_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWGk9sPgONiW"
      },
      "source": [
        "## Master Agent \n",
        "The master agent is responsible to keep a shared optimizer that updates its global network. Each worker agent will update both the global network and hte optimiser. Ee’ll use the AdamOptimizer with a very small learning rate for the CartPole task.\n",
        "\n",
        "One of the disadvantages of A3C is that some agents might end up working with an outdated version of the network, but that is not a huge problem.\n",
        "\n",
        "The master agent will run the train function to instantiate and start each of the agents. The master agent handles the coordinating and supervision of each agent. Each of these agents will run asynchronously. The workers will be implemented with a threads to make it more easy to understan.. Let's pretend GIL is not a think in Python (Global Interpreter Lock: a single python process can't run threads in parallel and can't use multiple cores). But it can run them concurrently during I/O bound operations.\n",
        "\n",
        "We will also create a \"play\" method to allow us to use a trained instance,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcFjT-qJOTqK"
      },
      "source": [
        "class MasterAgent(object):\n",
        "    def __init__(self,\n",
        "                 env_name='CartPole-v0',\n",
        "                 save_dir='.',\n",
        "                 algorithm='A3C',\n",
        "                 lr=0.0001,\n",
        "                 max_eps=100):\n",
        "        self.game_name = env_name\n",
        "        self.save_dir = save_dir\n",
        "        self.algorithm = algorithm\n",
        "        self.max_eps = max_eps\n",
        "\n",
        "        env = gym.make(self.game_name)\n",
        "        self.state_size = 0 # Complete here\n",
        "        self.action_size = 0 # Complete here\n",
        "        self.opt = 0 # Complete here\n",
        "        print(self.state_size, self.action_size)\n",
        "\n",
        "        self.global_model = ActorCriticModel(\n",
        "            self.state_size, self.action_size)  # global network\n",
        "        self.global_model(\n",
        "            tf.convert_to_tensor(\n",
        "                np.random.random((1, self.state_size)), dtype=tf.float32))\n",
        "\n",
        "    def train(self):\n",
        "        if self.algorithm == 'random':\n",
        "            random_agent = RandomAgent(self.game_name, self.max_eps)\n",
        "            random_agent.run()\n",
        "            return\n",
        "\n",
        "        res_queue = Queue()\n",
        "\n",
        "        workers = [\n",
        "            Worker(\n",
        "                self.state_size,\n",
        "                self.action_size,\n",
        "                self.global_model,\n",
        "                self.opt,\n",
        "                res_queue,\n",
        "                i,\n",
        "                game_name=self.game_name,\n",
        "                save_dir=self.save_dir,\n",
        "                max_eps=self.max_eps)\n",
        "            for i in range(multiprocessing.cpu_count())\n",
        "        ]\n",
        "\n",
        "        for i, worker in enumerate(workers):\n",
        "            print(\"Starting worker {}\".format(i))\n",
        "            worker.start()\n",
        "\n",
        "        moving_average_rewards = []  # record episode reward to plot\n",
        "        while True:\n",
        "            reward = res_queue.get()\n",
        "            if reward is not None:\n",
        "                moving_average_rewards.append(reward)\n",
        "            else:\n",
        "                break\n",
        "        [w.join() for w in workers]\n",
        "\n",
        "        plt.plot(moving_average_rewards)\n",
        "        plt.ylabel('Moving avg episode reward')\n",
        "        plt.xlabel('Step')\n",
        "        plt.show()\n",
        "        \n",
        "        \n",
        "    def play(self):\n",
        "        env = gym.make(self.game_name).unwrapped\n",
        "        state = env.reset()\n",
        "        model = self.global_model\n",
        "        model_path = os.path.join(self.save_dir,\n",
        "                                  'model_{}.h5'.format(self.game_name))\n",
        "        print('Loading model {}'.format(model_path))\n",
        "        model.load_weights(model_path)\n",
        "        done = False\n",
        "        step_counter = 0\n",
        "        reward_sum = 0\n",
        "\n",
        "        while not done:\n",
        "            env.render(mode='rgb_array')\n",
        "            policy, value = model(\n",
        "                tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
        "            policy = tf.nn.softmax(policy)\n",
        "            action = np.argmax(policy)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            reward_sum += reward\n",
        "            print(\"{}. Reward: {}, action: {}\".format(\n",
        "                step_counter, reward_sum, action))\n",
        "            step_counter += 1\n",
        "\n",
        "        env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RANaKppOZoP"
      },
      "source": [
        "\n",
        "## Memory\n",
        "To keep track of the hsitory, we’ll implement a Memory class. We want tokeep track of our actions, rewards, states of each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFIcHltEOdWx"
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def store(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6BCOArYOgj3"
      },
      "source": [
        "Worker Agent\n",
        "The most important part of the algorithm is the worker agent. To be asynchronous the worker agent inherits from the threading class, and we override the run method from Thread.\n",
        "\n",
        "Running the algorithm\n",
        "We’ll run all the threads for a set maximum number of episodes. This is where the third Actor part happens. Our agent will “act” according to our policy function, becoming the actor while the action is judged by the “critic,” which is our value function.\n",
        "\n",
        "this might seem complicated but all it only does:\n",
        "\n",
        "Get the action probability distribution based on the current frame\n",
        "Act according to policy\n",
        "The the agent will update the global model with the local gradient when the agent has taken the set number of steps (args.update_freq) or the agent has died then.\n",
        "\n",
        "After this it will all start again. The worker agent will repeat the process of resetting the network parameters to all of those in the global network, and repeating the process of interacting with its environment, computing the loss, and then applying the gradients to the global network.\n",
        "\n",
        "Computing the losses\n",
        "The worker agent calculates the losses to obtain gradients with respect to all of its own network parameters. This is where the last A of A3C come into play, the advantage. These are then applied to the global network. The losses are calculated as:\n",
        "\n",
        "Value Loss: L=∑(R — V(s))²\n",
        "Policy Loss: L = -log(𝝅(s)) * A(s)\n",
        "Where R is the discounted rewards, V our value function (of an input state), 𝛑 our policy function (of an input state as well), and A our advantage function. We use the discounted rewards to estimate our Q value since we don’t directly determine the Q value with A3C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xohbWqbOlOB"
      },
      "source": [
        "class Worker(threading.Thread):\n",
        "    # Set up global variables across different threads\n",
        "    global_episode = 0\n",
        "    # Moving average reward\n",
        "    global_moving_average_reward = 0\n",
        "    best_score = 0\n",
        "    save_lock = threading.Lock()\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_size,\n",
        "                 action_size,\n",
        "                 global_model,\n",
        "                 opt,\n",
        "                 result_queue,\n",
        "                 idx,\n",
        "                 max_eps=500,\n",
        "                 game_name='CartPole-v0',\n",
        "                 save_dir='/tmp',\n",
        "                 update_freq=20,\n",
        "                 gamma=0.99):\n",
        "        super(Worker, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.result_queue = result_queue\n",
        "        self.global_model = global_model\n",
        "        self.opt = opt\n",
        "        self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "        self.worker_idx = idx\n",
        "        self.max_eps = max_eps\n",
        "        self.game_name = game_name\n",
        "        self.env = gym.make(self.game_name).unwrapped\n",
        "        self.save_dir = save_dir\n",
        "        self.update_freq = update_freq\n",
        "        self.gamma = gamma\n",
        "        self.ep_loss = 0.0\n",
        "\n",
        "    def run(self):\n",
        "        total_step = 1\n",
        "        mem = Memory()\n",
        "        while Worker.global_episode < self.max_eps:\n",
        "            current_state = self.env.reset()\n",
        "            mem.clear()\n",
        "            ep_reward = 0.\n",
        "            ep_steps = 0\n",
        "            self.ep_loss = 0\n",
        "\n",
        "            time_count = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                logits, _ = self.local_model(\n",
        "                    tf.convert_to_tensor(\n",
        "                        current_state[None, :], dtype=tf.float32))\n",
        "                probs = tf.nn.softmax(logits)\n",
        "\n",
        "                action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
        "                new_state, reward, done, _ = self.env.step(action)\n",
        "                if done:\n",
        "                    reward = -1\n",
        "                ep_reward += reward\n",
        "                mem.store(current_state, action, reward)\n",
        "\n",
        "                if time_count == self.update_freq or done:\n",
        "                    # Calculate gradient wrt to local model. We do so by tracking the\n",
        "                    # variables involved in computing the loss by using tf.GradientTape\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        total_loss = self.compute_loss(done, new_state, mem,\n",
        "                                                       self.gamma)\n",
        "                    self.ep_loss += total_loss\n",
        "                    # Calculate local gradients\n",
        "                    grads = tape.gradient(total_loss,\n",
        "                                          self.local_model.trainable_weights)\n",
        "                    # Push local gradients to global model\n",
        "                    self.opt.apply_gradients(\n",
        "                        zip(grads, self.global_model.trainable_weights))\n",
        "                    # Update local model with new weights\n",
        "                    self.local_model.set_weights(\n",
        "                        self.global_model.get_weights())\n",
        "\n",
        "                    mem.clear()\n",
        "                    time_count = 0\n",
        "\n",
        "                    if done:  # done and print information\n",
        "                        Worker.global_moving_average_reward = \\\n",
        "                          record(Worker.global_episode, ep_reward, self.worker_idx,\n",
        "                                 Worker.global_moving_average_reward, self.result_queue,\n",
        "                                 self.ep_loss, ep_steps)\n",
        "                        # We must use a lock to save our model and to print to prevent data races.\n",
        "                        if ep_reward > Worker.best_score:\n",
        "                            with Worker.save_lock:\n",
        "                                print(\"Saving best model to {}, \"\n",
        "                                      \"episode score: {}\".format(\n",
        "                                          self.save_dir, ep_reward))\n",
        "                                self.global_model.save_weights(\n",
        "                                    os.path.join(\n",
        "                                        self.save_dir,\n",
        "                                        'model_{}.h5'.format(self.game_name)))\n",
        "                                Worker.best_score = ep_reward\n",
        "                        Worker.global_episode += 1\n",
        "                ep_steps += 1\n",
        "\n",
        "                time_count += 1\n",
        "                current_state = new_state\n",
        "                total_step += 1\n",
        "        self.result_queue.put(None)\n",
        "\n",
        "    def compute_loss(self, done, new_state, memory, gamma=0.99):\n",
        "        if done:\n",
        "            reward_sum = 0.  # terminal\n",
        "        else:\n",
        "            reward_sum = self.local_model(\n",
        "                tf.convert_to_tensor(new_state[None, :],\n",
        "                                     dtype=tf.float32))[-1].numpy()[0]\n",
        "\n",
        "        # Get discounted rewards\n",
        "        discounted_rewards = []\n",
        "        for reward in memory.rewards[::-1]:  # reverse buffer r\n",
        "            reward_sum = 0 # Complete here\n",
        "            discounted_rewards.append(reward_sum)\n",
        "        discounted_rewards.reverse()\n",
        "\n",
        "        logits, values = self.local_model(\n",
        "            tf.convert_to_tensor(np.vstack(memory.states), dtype=tf.float32))\n",
        "        # Get our advantages\n",
        "        advantage = tf.convert_to_tensor(\n",
        "            np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n",
        "        # Value loss\n",
        "        value_loss = advantage**2\n",
        "\n",
        "        # Calculate our policy loss\n",
        "        policy = 0 # Complete here\n",
        "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            labels=policy, logits=logits)\n",
        "\n",
        "        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=memory.actions, logits=logits)\n",
        "        policy_loss *= tf.stop_gradient(advantage)\n",
        "        policy_loss -= 0.01 * entropy\n",
        "        total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
        "        return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okx_9NCfOrNf"
      },
      "source": [
        "## Training A3C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8KMGQT5Oqci"
      },
      "source": [
        "master = MasterAgent(max_eps=500)\n",
        "master.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hci7jFgoOz7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}